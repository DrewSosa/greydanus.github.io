<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sam's Research Blog</title>
    <description>Science with a spirit of adventure.</description>
    <link>http://greydanus.github.io/</link>
    <atom:link href="http://greydanus.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 06 Sep 2016 18:07:54 -0400</pubDate>
    <lastBuildDate>Tue, 06 Sep 2016 18:07:54 -0400</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>The Art of Regularization **WORK IN PROGRESS**</title>
        <description>&lt;p&gt;Regularization seems fairly insignificant at first glance but it has a huge impact on deep models. I’ll use a one-layer neural network trained on the MNIST dataset to give an intuition for how common regularization techniques affect learning.&lt;/p&gt;

&lt;h2 id=&quot;mnist-classification&quot;&gt;MNIST Classification&lt;/h2&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/mnist.png&quot; width=&quot;30%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;MNIST training samples&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The basic idea here is to train a learning model to classify 28x28 images of handwritten digits (0-9). The dataset is relatively small (60k training examples) so it’s a classic benchmark for evaluating small models. TensorFlow provides a really simple API for loading the training data:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.examples.tutorials.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_data_sets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'MNIST_data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now &lt;code class=&quot;highlighter-rouge&quot;&gt;batch[0]&lt;/code&gt; holds the training data and &lt;code class=&quot;highlighter-rouge&quot;&gt;batch[1]&lt;/code&gt; holds the training labels. Making the model itself is really easy as well. For a fully-connected model without any regularization, we simply write:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# xlen is 28x28 = 784&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;y_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ylen is 10&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# no bias because meh&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The full code is available on &lt;a href=&quot;https://github.com/greydanus&quot;&gt;GitHub&lt;/a&gt;. I trained each model for 150,000 interations (well beyond convergence) to accentuate the differences between regularization methods.&lt;/p&gt;

&lt;h2 id=&quot;visualizing-regularization&quot;&gt;Visualizing regularization&lt;/h2&gt;

&lt;p&gt;Since the model uses a 784x10 matrix of weights to map pixels to the probabilities that they represent a given digit, we can visualize which pixels are the most important for predicting a given digit. For example, to visualize which pixels are the most important for predicting the digit ‘0’, we would take the first column of the weight matrix and reshape it into a 28x28 image.&lt;/p&gt;

&lt;h3 id=&quot;no-regularization&quot;&gt;No regularization&lt;/h3&gt;

&lt;p&gt;Can overfit by making the magnitudes of some weights very large, provided the dataset is small&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;# no additional code&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/noreg.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;&lt;b&gt;No regularization:&lt;/b&gt; these 'weight images' have a salt-and-pepper texture which suggests overfitting. Even so, the shadows of each of the digits are clearly visible&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;

&lt;p&gt;At each training step, dropout clamps some weights to 0, effectively stopping the flow of information through these connections. This causes the model to distribute computations across the entire network and prevents it from depending heavily on a subset features. In the MNIST example, dropout has a smoothing effect on the weights&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x = tf.nn.dropout(x, 0.5)&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/drop.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;&lt;b&gt;Dropout:&lt;/b&gt; these 'weight images' are much smoother because dropout prevents the model from placing too much trust in any one of its input features.&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;adaptive-weight-noise-work-in-progress-im-not-sure-if-this-is-actually-implemented-correctly&quot;&gt;Adaptive weight noise &lt;strong&gt;WORK IN PROGRESS: I’m not sure if this is actually implemented correctly&lt;/strong&gt;*&lt;/h3&gt;

&lt;p&gt;text&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;S_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;S_hat&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# make sure sigma matrix is positive&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gaussian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;noise_source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# draw each weight from a gaussian distribution&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/awn.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;&lt;b&gt;Adaptive weight noise:&lt;/b&gt; these 'weight images' are really different but the model reaches roughly the same training accuracy as the unregularized version.&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;l2-regularization&quot;&gt;L2 Regularization&lt;/h3&gt;

&lt;p&gt;L2 regularization penalizes weights with large magnitudes. Large weights are the most obvious symptom of overfitting, so it’s an obvious fix. It’s less obvious that L2 regularization actually has a Bayesian interpretation: since we initialize weights to very small values and L2 regression keeps these values small, we’re actually biasing the model towards the prior.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
		 &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_collection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphKeys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;REGULARIZATION_LOSSES&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/mag.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;&lt;b&gt;L2 regularization:&lt;/b&gt; these 'weight images' are very smooth and the digits are very clear. Even though the model has a better representation of what each digit 'looks like', the test accuracy is lower because messy/unusual examples are frequently misclassified.&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;weight-normalization&quot;&gt;Weight normalization&lt;/h3&gt;

&lt;p&gt;Normalizing the weight matrix is another way of keeping weights close to zero and, again, increases confidence in the prior. However, this form of regularization is not equivalent to L2 regularization and may behave differently in wider/deeper models.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l2_normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/regularization/norm.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;&lt;b&gt;Weight normalization:&lt;/b&gt; it's interesting to note that normalizing the weight matrix has the same effect as L2 regularization&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;comparison&quot;&gt;Comparison&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Type&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Test accuracy\(^1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Runtime\(^2\) (relative to first entry)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Min value\(^3\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Max value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;No regularization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;93.2%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.00&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1.95&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.64&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Dropout&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;89.5%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.49&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1.42&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Adaptive weight noise&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;93.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(\approx\)0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;L2 Regularization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.25&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-0.062&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.094&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Weight normalization&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;71.1%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.58&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-0.05&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.08&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;\(^1\)Accuracy doesn’t matter much at this stage because it changes dramatically as we alter hyperparameters and model width/depth. In fact, I deliberately made the hyperparameters very large to accentuate differences between each of the techniques. One thing to note is that Adaptive Weight Noise (AWN) achieves nearly the same accuracy as the unregularized model even though its weight matrix is very different.&lt;/p&gt;

&lt;p&gt;\(^2\)Since AWN solves for a \(\sigma\) and \(\mu\) for every single parameter, it ends up optimizing twice as many parameters which also roughly doubles runtime.&lt;/p&gt;

&lt;p&gt;\(^3\)L2 regularization and weight normalization are designed to keep all weights small, which is why the min/max values are small. Meanwhile, AWN produces an exclusively positive weight matrix because the Gaussian function is always positive.&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;Regularization matters! Not only is it a way of preventing overfitting; it’s also the easiest way to control what a model learns.&lt;/p&gt;

&lt;p&gt;In a simple model such as the one we used for this post, we can easily understand how regularization does this. How can we extend what we’ve learned here to larger, more complex models? First of all, many of the intuitions apply directly to models of any size.&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Sep 2016 07:00:00 -0400</pubDate>
        <link>http://greydanus.github.io/2016/09/05/regularization/</link>
        <guid isPermaLink="true">http://greydanus.github.io/2016/09/05/regularization/</guid>
        
        
      </item>
    
      <item>
        <title>Scribe: Realistic Handwriting with TensorFlow</title>
        <description>&lt;div class=&quot;imgcap_noborder&quot;&gt;
	&lt;img src=&quot;/assets/scribe/author.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;a-story&quot;&gt;A story&lt;/h2&gt;

&lt;p&gt;Like most elementary school kids in the 2000’s, I was a master of WordArt. I gleefully overused the big rainbow-colored fonts on everything from class essays to school newspaper articles. One thing that bothered me was the lack of good cursive fonts. Some years later I realized why: each letter in cursive gets shaped differently depending on what letters surround it. That makes mimicking someone’s cursive style with a computer – or even by hand - tricky. It’s the reason we still sign our names in cursive on legal documents.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/lucinda.png&quot; width=&quot;30%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;The best MS Word can do is make it curly&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In this post, I will demonstrate the power of deep learning by using it to generate human-like handwriting (including some cursive). This work is based on the methods from a famous 2014 paper, &lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt; by Alex Graves. With this post, I am releasing&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the &lt;a href=&quot;https://github.com/greydanus/scribe&quot;&gt;code&lt;/a&gt; I used to build and train the model&lt;/li&gt;
  &lt;li&gt;an &lt;a href=&quot;https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/sample.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; which explains the code in a step-by-step manner.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;building-the-graves-handwriting-model&quot;&gt;Building the Graves handwriting model&lt;/h2&gt;

&lt;h3 id=&quot;the-data&quot;&gt;The data&lt;/h3&gt;
&lt;p&gt;First let’s look at the data. I used the &lt;a href=&quot;http://www.fki.inf.unibe.ch/databases/iam-handwriting-database&quot;&gt;IAM Handwriting Database&lt;/a&gt; to train my model. As far as datasets go, it’s very small (less than 50 MB once parsed). A total of 657 writers contributed to the dataset and each has a unique handwriting style:&lt;/p&gt;

&lt;div class=&quot;imgcap_noborder&quot;&gt;
	&lt;img src=&quot;/assets/scribe/style_0.png&quot; width=&quot;100%&quot; /&gt;
	&lt;img src=&quot;/assets/scribe/style_1.png&quot; width=&quot;100%&quot; /&gt;
	&lt;img src=&quot;/assets/scribe/style_2.png&quot; width=&quot;100%&quot; /&gt;
	&lt;img src=&quot;/assets/scribe/style_3.png&quot; width=&quot;100%&quot; /&gt;
	&lt;img src=&quot;/assets/scribe/style_4.png&quot; width=&quot;100%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Five different handwriting styles. The average character contains about 25 points.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The data itself is a three-dimensional time series. The first two dimensions are the (x, y) coordinates of the pen tip and the third is a binary 0/1 value where 1 signifies the end of a stroke. Each line has around 500 pen points and an annotation of ascii characters.&lt;/p&gt;

&lt;h3 id=&quot;the-challenge&quot;&gt;The challenge&lt;/h3&gt;
&lt;p&gt;The data is three dimensional, sequential, and highly correlated both in space and in time. In other words, it’s a big ugly mess. It was originally meant for training online handwriting recognition models which learn that a series of pen points represents, say, the letter ‘a’:&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/stroke_to_ascii.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Online handwriting recognition (the original purpose of this dataset. RNN graphic courtesy of &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;colah&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Ok, that is a tough challenge but it can be done using out-of-the-box sequential models such as recurrent neural networks (RNNs). A much more difficult challenge is to reverse the process, ie. to train a model that takes the letter ‘a’ as an input and produces a series of points that we can connect to make the letter ‘a.’&lt;/p&gt;
&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/ascii_to_stroke.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Handwriting generation from ascii characters (a much harder challenge!).&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In order to make this happen, we’ll start with a recurrent neural network structure and then add some bells and whistles.&lt;/p&gt;

&lt;h3 id=&quot;a-beast-of-a-model&quot;&gt;A beast of a model&lt;/h3&gt;
&lt;p&gt;It’s easy to think of the Graves handwriting model is as three separate models. Each of these models can be trained using gradient backpropagation, so we basically stack them on top of each other like Legos and then train the whole beast from end-to-end. I’ll describe each model in turn and give an intuition for how they work together to generate handwriting.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/model_rolled.png&quot; width=&quot;40%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;The structure of the Graves handwriting model. Differentiable components have corners and &amp;lt;eos&amp;gt; is the end-of-stroke tag.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;The Long Short-Term Memory (LSTM) Cell.&lt;/strong&gt; At the core of the Graves handwriting model are three Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs). We could just as easily have used Gated Recurrent Units (GRUs), &lt;a href=&quot;https://arxiv.org/abs/1607.03474&quot;&gt;Recurrent Highway Networks&lt;/a&gt; (RHNs), or some other &lt;code class=&quot;highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cell. TensorFlow provides a &lt;a href=&quot;https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html&quot;&gt;built-in API&lt;/a&gt; for these models so it doesn’t really matter. If you don’t know what recurrent neural networks or LSTMs are, read &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;this post&lt;/a&gt; to see how they work and &lt;a href=&quot;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;this post&lt;/a&gt; to see what they can do.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/model_unrolled.png&quot; width=&quot;90%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;The recurrent structure allows the model to feed information forward from past iterations. Arrows represent how data flows through the model (gradients flow backwards)&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;These networks use a differentiable form of memory to keep track of time-dependent patterns in data. LSTMs, for example, use three different tensors to perform ‘erase’, ‘write’, and ‘read’ operations on a ‘memory’ tensor: the \(f\), \(i\), \(o\), and \(C\) tensors respectively (&lt;a href=&quot;https://mr-london.herokuapp.com/index&quot;&gt;more&lt;/a&gt; on this). For the purposes of this post, just remember that RNNs are extremely good at modeling sequential data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Mixture Density Network (MDN).&lt;/strong&gt; Think of Mixture Density Networks as neural networks which can measure their own uncertainty. Their output parameters are \(\mu\), \(\sigma\), and \(\rho\) for several multivariate Gaussian components. They also estimate a parameter \(\pi\) for each of these distributions. Think of \(\pi\) as the probability that the output value was drawn from that particular component’s distribution. Last year, I wrote an &lt;a href=&quot;https://nbviewer.jupyter.org/github/greydanus/adventures/blob/master/mixture_density/mdn.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; about MDNs.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/MDN.png&quot; width=&quot;50%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;The importance of π: what is the probability the red point was drawn from each of the three distributions?&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Since MDNs parameterize probability distributions, they are a great way to capture randomness in the data. In the handwriting model, the MDN learns to how messy or unpredictable to make different parts of handwriting. For example, the MDN will choose Gaussian with diffuse shapes at the beginning of strokes and Gaussians with peaky shapes in the middle of strokes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Attention Mechanism.&lt;/strong&gt; Imagine that we want our model to write &lt;i&gt;‘You know nothing Jon Snow.’&lt;/i&gt; In order to get the information about which characters make up this sentence, the model uses a differentiable attention mechanism. In technical terms, it is a Gaussian convolution over a &lt;a href=&quot;https://en.wikipedia.org/wiki/One-hot&quot;&gt;one-hot&lt;/a&gt; ascii encoding. Think of this convolution operation as a soft window through which the handwriting model can look at a small subset of characters, ie. the letters ‘kn’ in the word ‘know’. Since all the parameters of this window are differentiable, the model learns to shift the window from character to character as it writes them&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/diag_window.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;A time series plot of the window's position. The vertical axis is time (descending) and the horizontal axis is the sequence of ascii characters that the model is drawing.&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/onehot_window.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;A time series of one-hot encodings produced by the attention mechanism. Again, the vertical axis is time. The horizontal axis is what the model sees when it looks through the soft window.&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;The model learns to control the window parameters remarkably well. For example, the bright stripes in the first plot are the model’s way of encoding the end of a pen stroke. We never hard-coded this behavior!&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;It works! After a couple hours on a Tesla K40 GPU, the model generates legible letters and after a day or so it writes sentences with only a few small errors. Even though most of the training sequences were 256 points long, I was able to sample sequences of up to 750.&lt;/p&gt;

&lt;div class=&quot;imgcap_noborder&quot;&gt;
	&lt;img src=&quot;/assets/scribe/jon_print.png&quot; width=&quot;100%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;You know nothing Jon Snow&lt;/div&gt;
	&lt;img src=&quot;/assets/scribe/jon_cursive.png&quot; width=&quot;100%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;cursive is still hard :(&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Since the model’s MDN cap predicts the pen’s \((x,y)\) coordinates by drawing them from a Gaussian distribution, we can modify that distribution to make the handwriting cleaner or messier. I followed Alex Graves’ example by introducing a ‘bias’ term \(b\) which redefines the \(\pi\) and \(\sigma\) parameters according to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma^{j}_{t} = \exp \left( \hat \sigma (1+b) \right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^{j}_{t} = \frac{ \exp \left( \hat \pi (1+b) \right) } { \sum_{j'=1}^{M} \exp \left( \hat \pi (1+b) \right) }&lt;/script&gt;

&lt;p&gt;To better understand what is happening here, check out my &lt;a href=&quot;https://nbviewer.jupyter.org/github/greydanus/scribe/blob/master/sample.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; or the &lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;original paper&lt;/a&gt;. Below are results for \(b={0.5,0.75,1.0}\)&lt;/p&gt;

&lt;div class=&quot;imgcap_noborder&quot;&gt;
	&lt;img src=&quot;/assets/scribe/bias-1.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;bias = 1.0&lt;/div&gt;
	&lt;img src=&quot;/assets/scribe/bias-0.75.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;bias = 0.75&lt;/div&gt;
	&lt;img src=&quot;/assets/scribe/bias-0.5.png&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;bias = 0.5&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;the-lego-effect&quot;&gt;The Lego Effect&lt;/h2&gt;
&lt;p&gt;Deep learning used to be divided neatly according to algorithms. There were convolutional neural networks (ConvNets) for image recognition, RNNs for sequence analysis, and Deep-Q or Policy Gradient Networks for Reinforcement Learning. Some of the most exciting papers in the past year combine several of these algorithms in a single differentiable model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Lego Effect is a new trend of stacking together several differentiable models and training them all end-to-end so that each solves a subset of the task&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The Graves handwriting model is one of the first examples of the Lego Effect. As I explained above, it’s actually a combination of three different types of models. The RNN cell learns to reproduce sequences of pen points, the MDN models randomness and style in the handwriting, and the attention mechanism tells the model what to write.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/scribe/densecap.jpg&quot; width=&quot;60%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Image captioning using a ConvNet + LSTM model. More info &lt;a href=&quot;https://cs.stanford.edu/people/karpathy/densecap/&quot;&gt;here&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;There are many other examples of the Lego Effect in the wild. Andrej Karpathy and Justin Johnson combined ConvNets with an LSTM to &lt;a href=&quot;https://cs.stanford.edu/people/karpathy/densecap/&quot;&gt;generate image captions&lt;/a&gt;. AI researchers regularly use ConvNets in their reinforcement learning models to solve vision-based games like &lt;a href=&quot;https://www.youtube.com/watch?v=TmPfTpjtdgg&quot;&gt;Atari Breakout&lt;/a&gt;. Alex Graves and some other DeepMind researchers combined ConvNets with policy gradients in &lt;a href=&quot;https://arxiv.org/abs/1406.6247&quot;&gt;Recurrent Models of Visual Attention&lt;/a&gt; to do gradient-based training on a network which makes non-differentiable decisions.&lt;/p&gt;

&lt;p&gt;These sorts of papers will only become more common in coming months. One area of rapid research which promises to benefit from the Lego Effect is differentiable long term memory. Several well-established models have short term memory (LSTMs, Gated Recurrent Units (GRUs), Recurrent Highway Networks) but only a few research-only models (&lt;a href=&quot;https://arxiv.org/abs/1410.5401&quot;&gt;Neural Turing Machines&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1605.07427v1&quot;&gt;Hierarchical Memory Networks&lt;/a&gt;) have the neural equivalent of a hard drive. The fact is that trainable long term memory is really tough to implement! There are some early signs of success in this area that use the Lego Effect. One really exciting paper, &lt;a href=&quot;https://arxiv.org/pdf/1505.00521v3.pdf&quot;&gt;Reinforcement Learning Neural Turing Machines&lt;/a&gt; combines reinformement learning, recurrent networks, and a big bag of DL tricks to tackle the challenge.&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h2&gt;

&lt;p&gt;Deep learning never started out as a way to generate handwriting. The fact that I was able to achieve state of the art in this task after only a few slight adjustments to an LSTM is what has made the field so explosive and successful. The strength of deep learning models is their generality. For example, by simply increasing the number of parameters and changing my training data, I could turn this model into a text-to-speech generator. The possibilities are limitless.&lt;/p&gt;

&lt;p&gt;I hope this project gives you a sense of why deep learning is both extremely cool and brimming with potential. As a scientist by training, I see deep learning as a powerful tool for scientific discovery.&lt;/p&gt;
</description>
        <pubDate>Sun, 21 Aug 2016 06:00:00 -0400</pubDate>
        <link>http://greydanus.github.io/2016/08/21/handwriting/</link>
        <guid isPermaLink="true">http://greydanus.github.io/2016/08/21/handwriting/</guid>
        
        
      </item>
    
      <item>
        <title>What is Deep Learning?</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Deep learning has solved vision…” - Demis Hassabis (Google DeepMind)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When &lt;a href=&quot;https://en.wikipedia.org/wiki/Demis_Hassabis&quot;&gt;Demis Hassabis&lt;/a&gt; opened his lecture at the 2015 CERN Data Science conference with this claim, I blinked in disbelief. Hundreds of brilliant scientists have spent their lives pondering the mysteries of the human eye. How could he make that claim so casually? Determined to prove him wrong, I looked at the latest computer vision research and… indeed, certain areas are &lt;a href=&quot;https://arxiv.org/abs/1502.01852v1&quot;&gt;at human performance&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/what_is/neural_style.png&quot; width=&quot;70%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Deep learning magic: a ConvNet-based architecture can capture artistic style and apply it to a new image (from &lt;a href=&quot;https://github.com/jcjohnson/neural-style&quot;&gt;jcjohnson&lt;/a&gt;)&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In fact, deep learning can solve more than vision. It can &lt;a href=&quot;http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html&quot;&gt;defeat the world champion in Go&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1406.1078&quot;&gt;translate text&lt;/a&gt;, and paint &lt;a href=&quot;https://github.com/jcjohnson/neural-style&quot;&gt;like Van Gough&lt;/a&gt;. It’s also been known to &lt;a href=&quot;https://www.youtube.com/user/stanfordhelicopter&quot;&gt;fly helicopters&lt;/a&gt; extremely well, &lt;a href=&quot;http://www.nature.com/articles/srep17573&quot;&gt;fold protiens&lt;/a&gt;, and &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v42/sado14.html&quot;&gt;classify subatomic particles&lt;/a&gt;. Obviously, deep learning is awesome… but what exactly is it? After being obsessed with this field for more than a year, I should have a concise and satisfying answer. Strangely, I have three.&lt;/p&gt;

&lt;h2 id=&quot;biological-picture&quot;&gt;Biological picture&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;“A lot of the motivation for deep nets did come from looking at the brain…” - Geoffrey Hinton&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The field of deep learning got started when scientists tried to approximate certain circuits in the brain. For example, the Convolutional Neural Network (ConvNet) – a cornerstone of modern computer vision – was inspired by &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/4966457&quot;&gt;a paper&lt;/a&gt; about neurons in the monkey striate cortex. Another example is the field of Reinforcement Learning – the hottest area of AI right now - which was built on our understanding of how the brain processes rewards.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/what_is/bio_vs_dl.png&quot; width=&quot;80%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;The architecture of neural networks is inspired by connections between neurons&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Skeptics claim that deep learning stretches the brain metaphor too far. Some of their critiques are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Scale:&lt;/em&gt; The largest neural nets have around 100 million ‘connections’. The human brain has over a 100 &lt;em&gt;trillion&lt;/em&gt; connections. With faster GPUs and cluster computing, the size of deep learning models has increased by orders of magnitude in the past few years. This pattern will probably continue.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Backprop:&lt;/em&gt; Some scientists believe that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;&gt;backpropagation algorithm&lt;/a&gt; has no biological correlate. Google DeepMind researchers &lt;a href=&quot;http://biorxiv.org/content/early/2016/06/13/058545&quot;&gt;disagree&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Architecture:&lt;/em&gt; Certain areas of the brain are able to store really complicated memories, perform symbolic operations, and build models of the outside world. These are things that state-of-the art deep learning cannot do.  There’s been a push in research to &lt;a href=&quot;https://arxiv.org/abs/1604.00289&quot;&gt;change this&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mathematical-picture&quot;&gt;Mathematical picture&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;“These adjustable parameters, often called weights, are real numbers that can be seen as ‘knobs’ that define the input–output function of the machine” - &lt;a href=&quot;http://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html&quot;&gt;2015 Nature Review&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mathematicians will tell you that deep learning is just complicated regression. Regression is the art of finding a function that explains the relationship between an input and an output. The simplest example is from middle school math when we used two points on a Cartesian plane to find the variables \(m\) and \(b\) in \(f(x)=mx+b\).&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/what_is/regression.png&quot; width=&quot;100%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Deep learning as a type of regression (images from 2015 &lt;a href=&quot;https://arxiv.org/abs/1409.0575&quot;&gt;ImageNet paper&lt;/a&gt;)&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Deep learning is the same thing except \(f(x)\) can be arbitrarily complex and nonlinear. The input could be the pixels of an image and the outputs a caption which describes the image. The input could be a sentence of French and the output could be a sentence in English. Obviously, functions that perform these mappings must be extremely complicated. In fact, deep learning models often have millions of free parameters instead of just an \(m\) and a \(b\). That said, it’s just regression.&lt;/p&gt;

&lt;h2 id=&quot;computer-science-picture&quot;&gt;Computer science picture&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;“The cost of a world-class deep learning expert was about the same as a top NFL quarterback prospect” - Peter Lee (Microsoft Research)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;The tech industry.&lt;/strong&gt; Tech companies love to build hype around vague keyphrases such as ‘cloud’, ‘big data’, and ‘machine learning.’ Inevitably, ‘deep learning’ will join the list. There are really two approaches to deep learning in tech. The first says: this gets great results and makes us a profit – let’s pour a ton of money into it. The second says: this is the cure-all, next-big-thing, singularity-inducing pinnacle of computer science - let’s pour a ton of money into it. There are a few &lt;a href=&quot;https://openai.com/about/&quot;&gt;notable exceptions&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;imgcap_noborder&quot;&gt;
	&lt;img src=&quot;/assets/what_is/trends.png&quot; width=&quot;150%&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Research.&lt;/strong&gt; Even in academia, researchers tend to prioritize results over understanding; deep learning theory lags woefully behind practice. If you do not have a background in computer science, you’ll get the best answers to the question “What can deep learning do?” and the worst answers to the question, “What is deep learning?” from people in tech.&lt;/p&gt;

&lt;h2 id=&quot;closing-thoughts&quot;&gt;Closing thoughts&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;“What I cannot create I do not understand” - Richard Feynman&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Deep learning lets us create some really exciting projects in the areas of computer vision, translation, and strategy. Richard Feynman once said, “What I cannot create I do not understand,” and this really plays to our advantage here. Now that we are able to create these projects, they can help us understand puzzles such as human vision, the structure of language, and decision making on an entirely new level.&lt;/p&gt;

&lt;div class=&quot;imgcap&quot;&gt;
	&lt;img src=&quot;/assets/what_is/griffiths.png&quot; width=&quot;100%&quot; /&gt;
	&lt;div class=&quot;thecap&quot; style=&quot;text-align:center&quot;&gt;Comparing the way humans and ConvNets represent the same set of images can teach us a lot about vision (from &lt;a href=&quot;https://arxiv.org/abs/1608.02164&quot;&gt;Adapting Deep Network Features to Capture Psychological Representations&lt;/a&gt;)&lt;/div&gt;
&lt;/div&gt;
</description>
        <pubDate>Fri, 05 Aug 2016 07:00:00 -0400</pubDate>
        <link>http://greydanus.github.io/2016/08/05/what-is/</link>
        <guid isPermaLink="true">http://greydanus.github.io/2016/08/05/what-is/</guid>
        
        
      </item>
    
  </channel>
</rss>
