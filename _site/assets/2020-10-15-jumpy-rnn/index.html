<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jumpy Recurrent Neural Networks</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="A science blog by Sam Greydanus">
    <link rel="canonical" href="http://localhost:4000/assets/2020-10-15-jumpy-rnn/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Natural Intelligence posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72311215-6', 'auto');
      ga('send', 'pageview');

    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

</head>


    <body>

    <header class="site-header">

  <div class="wrap">

    <div style="float:left; margin-top:11px; margin-right:10px;">
    <img src="/assets/oak.png" width="50px">
    </div>

    <div style="float:left; margin-top:4px; margin-right:10px;">
      <a class="site-title" style="padding-top:0px; padding-bottom:0px;" href="/ ">Natural Intelligence</a>
      <p style="padding-bottom:6px; font-size:14px">A science blog by Sam Greydanus</p>
    </div>

    <!-- <a class="site-title" style="padding-top:8px; font-size:28px" href="/ ">Natural Intelligence</a> -->
    
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger" style="margin-bottom:0px; padding-top:10px; font-size:13px">
<!--         <a class="page-link" href="/ ">Home</a> -->
<!--         
          <a class="page-link" href="/assets/2020-10-15-jumpy-rnn/">Jumpy Recurrent Neural Networks</a>
        
          
        
          <a class="page-link" href="/papers/">Interesting papers</a>
         -->
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="https://greydanus.github.io">Home</a>
        <a class="page-link" style="margin-bottom: 0px; padding-bottom:0px;" href="https://greydanus.github.io/about.html">About me</a>
      </div>
    </nav>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Jumpy Recurrent Neural Networks</h1>
    <p class="meta">Oct 15, 2020 • Sam Greydanus, Stefan Lee, and Alan Fern</p>
  </header>

  <article class="post-content">
  <div>
	<style>
		#linkbutton:link, #linkbutton:visited {
		  background-color: rgb(180,180,180);
		  border-radius: 4px;
		  color: white;
		  padding: 6px 0px;
		  width: 150px;
		  text-align: center;
		  text-decoration: none;
		  display: inline-block;
		  text-transform: uppercase;
		  font-size: 13px;
		  margin: 8px;
		}

		#linkbutton:hover, #linkbutton:active {
		  background-color: rgba(160,160,160);
		}

		.playbutton {
		  background-color: rgb(148, 196, 146);
		  border-width: 0;
		  /*background-color: rgba(255, 130, 0);*/
		  border-radius: 4px;
		  color: white;
		  padding: 5px 8px;
		  /*width: 60px;*/
		  text-align: center;
		  text-decoration: none;
		  text-transform: uppercase;
		  font-size: 12px;
		  /*display: block;*/
		  /*margin-left: auto;*/
		  margin: 8px 0px;
		  margin-right: auto;
		  min-width:60px;
		}

		.playbutton:hover, .playbutton:active {
		  background-color: rgb(128, 176, 126);
		}
	</style>
</div>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_sim" style="width:100%;min-width:250px;">
    	<source src="/assets/jumpy-rnn/video_simulator.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_sim_button" onclick="playPauseSim()">Play</button> 
    <div style="text-align: left;margin-left:10px;margin-right:10px;">Using model-based planning to play pool/billiards. The goal is to impart the tan cue ball with an initial velocity so as to move the blue ball to the black target.</div>
  </div>
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_base" style="width:100%;min-width:250px;">
    	<source src="/assets/jumpy-rnn/video_base.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_base_button" onclick="playPauseBase()">Play</button> 
    <div style="text-align:left;margin-left:10px;margin-right:10px;">A baseline RNN trained on billiards dynamics can also be used for model-based planning. It's inefficient because it has to "tick" at a constant rate.</div>
  </div>
   <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;">
    <video id="video_jumpy" style="width:100%;min-width:250px;">
    	<source src="/assets/jumpy-rnn/video_jumpy.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_jumpy_button" onclick="playPauseJumpy()">Play</button> 
    <div style="text-align:left;margin-left:10px;margin-right:10px;">By contrast, a Jumpy RNN trained on the same task can perform planning in many fewer steps by jumping over spans of time where motion is predictable.</div>
  </div>
</div>

<script> 
function playPauseSim() { 
  var video = document.getElementById("video_sim"); 
  var button = document.getElementById("video_sim_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseBase() { 
  var video = document.getElementById("video_base"); 
  var button = document.getElementById("video_base_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseJumpy() { 
  var video = document.getElementById("video_jumpy"); 
  var button = document.getElementById("video_jumpy_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 
</script>

<div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;">
	<a href="https://openreview.net/pdf?id=4c3WeBTErrE" id="linkbutton" target="_blank">Read the paper</a>
	<a href="" id="linkbutton" target="_blank">Run in browser</a>
	<a href="https://github.com/greydanus" id="linkbutton" target="_blank">Get the code</a>
</div>

<h2 id="change-it-is-said-happens-slowly-and-then-all-at-once">Change, it is said, happens slowly and then all at once…</h2>

<!-- Change, it is said, happens slowly and then all at once. -->
<p>Billiards balls move across a table before colliding and changing trajectories; water molecules cool slowly and then undergo a rapid phase transition into ice; and economic systems enjoy periods of stability interspersed with abrupt market downturns. That is to say, many time series exhibit periods of relatively homogeneous change divided by important events. Despite this, recurrent neural networks (RNNs) – a popular type of time series model – treat time in uniform intervals. In doing so, they are forced to spend much of their time stepping through periods of relatively predictable change.</p>

<p>In this post, I will introduce a simple RNN variant that can jump over long time intervals while retaining the ability to produce fine-grained updates when necessary. This “Jumpy RNN” model can make predictions that are either one second in the future or twenty seconds in the future depending on the context.</p>

<p>Whereas most RNNs maintain a hidden state which summarizes their belief about where they are in the world, our model maintains both a hidden state \(h\) <em>and</em> a hidden velocity \(\dot h\). Then, by assuming that the hidden state changes linearly over a certain duration of time, we can predict any state within that duration by multiplying the hidden velocity by change in time and adding it to the hidden state: \(h(t) = h + \dot h \Delta t\). In effect, this new model now <em>predicts a sequence of linear dynamics functions and durations over which they are valid</em>. By doing so, it can summarize a long span of homogeneous change in one go. Alternatively, it can break that span into smaller bits in order to improve accuracy.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/jumpy-rnn/hero.png" style="width:100%" />
</div>

<p>In order to compare our model to regular RNNs, we used both of them to model a series of simple physics problems including the collisions of two pool/billiards balls. We found that our jumpy model was able to learn these patterns at least as well as the baseline while using a fraction of the forward simulation steps. This makes it a great candidate for model-based planning because it can predict the outcome of taking an action much more quickly than a baseline model. And since the hidden-state dynamics are continuous in time, we can solve for the hidden state at any arbitrary point in time. This allows us to simulate the dynamics of, say, billiards, at a higher temporal resolution than the original simulator:</p>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;padding-right:10px;">
    <video id="video_jumpy2" style="width:100%;min-width:250px;">
    	<source src="/assets/jumpy-rnn/video_jumpy.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_jumpy2_button" onclick="playPauseJumpy2()">Play</button> 
    <div style="text-align:left;margin-left:10px;margin-right:10px;">Using a Jumpy RNN to predict the same trajectory.</div>
  </div>
  <div style="width:32%; min-width:250px; display: inline-block; vertical-align: top;text-align:center;">
    <video id="video_interp" style="width:100%;min-width:250px;">
    	<source src="/assets/jumpy-rnn/video_interp.mp4" type="video/mp4" />
    </video>
    <button class="playbutton" id="video_interp_button" onclick="playPauseInterp()">Play</button> 
    <div style="text-align:left;margin-left:10px;">By interpolating between hidden states predicted by the Jumpy RNN, we can obtain dynamics at a higher temporal resolution than that of the original simulator.</div>
  </div>
</div>

<script> 
function playPauseSim2() { 
  var video = document.getElementById("video_sim2"); 
  var button = document.getElementById("video_sim2_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseJumpy2() { 
  var video = document.getElementById("video_jumpy2"); 
  var button = document.getElementById("video_jumpy2_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 

function playPauseInterp() { 
  var video = document.getElementById("video_interp"); 
  var button = document.getElementById("video_interp_button");
  if (video.paused) {
    video.play();
	button.textContent = "Pause";}
  else {
    video.pause(); 
	button.textContent = "Play";}
} 
</script>

<p>In summary, Jumpy RNNs represent an important improvement over regular RNNs for several important reasons. But before we discuss those reasons, we need to begin by talking about what regular RNNs are good at, and why they are worth improving upon.</p>

<h2 id="the-value-of-rnns">The Value of RNNs</h2>

<p>Recurrent neural networks are interesting because they can learn complex, long-range structure in time series data simply by predicting one point at a time. For example, if you train them on sequences of words, you can use them to translate from one language to another, or to generate possible completions to a prompt. And if you train them on observations of a robot arm, you can use them to generate realistic paths that the arm might take in the future. One of the things that makes these models so flexible is that they use a hidden vector <em>h</em> to store memories of past observations. And they can <em>learn</em> to read, write, and erase information from h in order to make accurate predictions about the future. This means that RNNs are quite good (although simplistic) models for how brains learn to use memory for such things. Here is an incomplete list of things people have trained RNNs to do:</p>
<ul>
  <li>•\(~~\)Translate text from one language to another (think Google Translate)</li>
  <li>•\(~~\)Control a robot hand in order to solve a Rubik’s Cube</li>
  <li>•\(~~\)Defeat professional human gamers in StarCraft</li>
  <li>•\(~~\)Caption images</li>
  <li>•\(~~\)Generate realistic handwriting</li>
  <li>•\(~~\)Convert text to speech</li>
  <li>•\(~~\)Convert speech to text</li>
  <li>•\(~~\)Compose simple songs</li>
  <li>•\(~~\)…and much more</li>
</ul>

<h2 id="the-limitations-of-rnns">The Limitations of RNNs</h2>

<p><strong>Uniform ticks.</strong> I’ve been interested in RNNs for a long time, but one aspect of their design has always bothered me: they can only tick at one uniform rate, sort of like a clock. At each tick they make one observation of the world, perform one read-erase-write on their memory, and send one output message. This seems too rigid. We wouldn’t open our eyes, close them, think, then act, and repeat. This would be silly because sometimes we need to think fast and other times we need to think slow. For us, time is a very dynamic thing.</p>

<p>One of the great things about being able to think in this manner is that it helps us plan. For example, we can jump to three months in the future when we will be traveling to another country, realize that we’ll need our passport updated, and then make a short-term plan for how to do that. Our short-term plan might be: stop at the Post Office to get passport photos after work. And in order to pay for those photos, grab some extra cash from the piggy bank. And so the simple act of putting a $20 bill in our wallets can have many layers of planning behind it. What’s more is that planning is happening across many time scales. Our brain does this by jumping from one event to another without thinking much about the intervening duration, and this is something that models like RNNs aren’t great at doing.</p>

<p><strong>Discrete time steps.</strong> Another issue with RNNs is that they perceive time as a series of discrete “time steps” that connect neighboring states. Since time is actually a continuous variable – it has a definite value even in between RNN ticks – we really should use models that treat it as such. In other words, when we ask our model what the world looked like at time \( t=1.42\) seconds, it should not have to locate the two ticks that are nearest in time and then interpolate between them, as is the case with RNNs. Rather, it should be able to give a well-defined answer. Models like Neural ODEs (which are closely related to this work) take a step in this direction, but it’s still an underexplored area of research.</p>

<h2 id="our-results">Our Results</h2>

<p>Our work on Jumpy RNNs was an attempt to fix these issues. Our model can jump over different durations of time and this allows it to tick more often when a lot is happening and less often otherwise. As we explained in the introduction, Jumpy RNNs are different from regular RNNs in that they predict a hidden state velocity in addition to a hidden state. Taken together, these two quantities represent a linear dynamics function in the RNN’s latent space. A second modification we make is to have the model predict the duration of time \(\Delta t\) over which that dynamics function is valid. In some cases, when change is happening at a constant rate, this value can be quite large. For example, when we create a toy dataset of perfectly linear motion, our model learns to summarize the trajectory in a single step.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/jumpy-rnn/lines.png" style="width:100%" />
</div>

<p>It’s worth noting that the way a system changes in time is only linear with respect to a particular coordinate system. For example, an object undergoing constant circular motion has nonlinear dynamics when we use Cartesian coordinates, but linear dynamics when we use Polar coordinates. That’s why physicists use different coordinates to describe different physical systems: the best coordinates are those that are maximally linear with respect to the dynamics.</p>

<p>Since a Jumpy RNN forces dynamics to be linear in latent space, the encoder and decoder layers naturally learn to transform input data into a basis where the dynamics are linear. For example, when we train our model on a dataset of circular trajectories represented in Cartesian coordinates, it learns to summarize such trajectories in a single step as well. This implies that circular motion is linear in the model’s latent space, and that it has effectively learned a Cartesian-to-Polar conversion.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/jumpy-rnn/circles.png" style="width:100%" />
</div>

<p>Our model can learn more complicated change-of-basis functions as well. Later in the paper, we trained our model on pixel observations of two billiards balls. The pixel “coordinate system” is extremely nonlinear with respect to the linear motion of the two balls. And yet our model was able to predict the dynamics of the system <em>more effectively</em> than the baseline model using three times fewer “ticks.” This jumpiness implies that our model found a basis in which the billiards dynamics were linear for significant durations of time – something that would never happen in a pixel basis.</p>

<div class="imgcap_noborder" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/jumpy-rnn/pixel_billiards.png" style="width:100%" />
</div>

<p>In fact, we suspect that forcing dynamics to be linear in latent space actually <em>biased</em> our model to find linear dynamics. We hypothesize that the baseline model performed worse on this task because it had no such inductive bias.</p>

<h2 id="planning">Planning</h2>

<p>One of the reasons we originally set out to build a Jumpy RNN was that we wanted to use it for planning. We were struck by the fact that many events one would want to plan over – collisions, in the case of billiards – are separated by variable durations of time. We suspected that a jumpy model would be particularly effective at planning in such scenarios.</p>

<p>In order to test this hypothesis, we tested our model against a baseline RNN on a simple planning task in the billiards environment. The goal was to impart one ball, the “cue ball” (visualized in tan) with an initial velocity such that it would collide with the second ball and the second ball would ultimately enter a target region (visualized in black).</p>

<p>We found that our model used half the wall time of the baseline and produced plans with a higher probability of success. These results are preliminary – and part of ongoing work – but they do confirm our hypothesis that the jumpy model would be useful for model-based planning.</p>

<h2 id="related-work">Related Work</h2>

<p><strong>Other continuous-time models.</strong> One challenge in training neural ODEs is that they must be continuously solved even when no observations occur, making forward passes take 60\% - 120\% longer than standard RNNs \citep{rubanova2019latent}. Since the integration speed of a Neural ODE is proportional to the curvature of the hidden state dynamics (higher curvature requires more steps), one can improve speed and efficiency by regularizing that curvature. \cite{finlay2020train} take a step in this direction. Even so, the number of time steps per function evaluation tends to be less than one. Our model resembles Neural ODEs in that it parameterizes the derivative of a hidden state. However, we make the simplifying assumption that the hidden state dynamics are linear for long stretches of time. This makes our model extremely efficient to integrate over long spans of time – more efficient, in fact, than a baseline RNN. This jumpy behavior is what our model is designed for, and where it excels compared to Neural ODEs.</p>

<p><strong>Other approaches to temporal abstraction.</strong> There are other RNN-based models designed with temporal abstraction in mind. \cite{koutnik2014clockwork} proposed dividing an RNN internal state into groups and only performing cell updates on the $i^{th}$ group after $2^{i-1}$ time steps. More recent works have aimed to make this hierarchical structure more adaptive, either by data-specific rules \citep{ling2015character} or by a learning  mechanism \citep{chung2019hierarchical}. All of these hierarchical recurrent models can model data at different timescales, but they all \textit{must} perform cell updates at every time step in a sequence, and hence cannot jump over regions of homogeneous change.</p>

<p>Recent work by \cite{gregor2018temporal} in the context of reinforcement learning develops a jumpy planning model which does not use an RNN cell or perform continuous interpolation of latent states. Another relevant work in the field of RL is Embed to Control by \cite{watter2015embed}. Their work is similar to ours in that they assume that dynamics are linear in latent space and they train their model on several dynamics problems, some of them with pixel observations. But unlike our work, their model performs inference over discrete, uniform time steps and does not learn a jumpy behavior.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Having achieved widespread use in commercial and academic settings, RNNs are already a useful tool. But even though they are useful tools, they still have fundamental limitations. In this paper, we reckoned with the fact that they can only forecast the future in discrete, uniform time steps. In order to make RNNs more useful in more contexts, it is essential to find solutions to such restrictions. With this in mind, we proposed a Jumpy RNN model which can skip over long durations of comparatively homogeneous change and focus on pivotal events as the need arises. We hope that this line of work will expand the possible uses of RNNs and make them capable of representing time in a more efficient and flexible manner.</p>

<h2 id="footnotes">Footnotes</h2>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <!-- disqus comments -->
 
  <div id="disqus_thread"></div>
  <script>
      /**
       *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
       *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
       */
      /*
      var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() {  // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          
          s.src = '//greydanus-blog.disqus.com/embed.js';
          
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  


  
</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <!-- <h2 class="footer-heading">Natural Intelligence</h2> -->

    <div class="footer-col-1 column">
      <ul>
        <li>Natural Intelligence</li>
        <!-- <li><a href="mailto:greydanus.17(at)gmail.com">greydanus.17(at)gmail.com</a></li> -->
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/greydanus">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">greydanus</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/samgreydanus">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
              </svg>
            </span>
            <span class="username">samgreydanus</span>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">A science blog by Sam Greydanus</p>
    </div>

  </div>

</footer>


    </body>
</html>