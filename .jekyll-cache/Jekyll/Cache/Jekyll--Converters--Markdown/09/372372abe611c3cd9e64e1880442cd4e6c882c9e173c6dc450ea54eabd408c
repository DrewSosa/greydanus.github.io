I"ò,<div>
	<style>
		#linkbutton:link, #linkbutton:visited {
		  background-color: rgb(180,180,180);
		  border-radius: 4px;
		  color: white;
		  padding: 6px 0px;
		  width: 150px;
		  text-align: center;
		  text-decoration: none;
		  display: inline-block;
		  text-transform: uppercase;
		  font-size: 13px;
		  margin: 8px;
		}

		#linkbutton:hover, #linkbutton:active {
		  background-color: rgba(160,160,160);
		}

		.playbutton {
		  background-color: rgb(148, 196, 146);
		  border-width: 0;
		  /*background-color: rgba(255, 130, 0);*/
		  border-radius: 4px;
		  color: white;
		  padding: 5px 8px;
		  /*width: 60px;*/
		  text-align: center;
		  text-decoration: none;
		  text-transform: uppercase;
		  font-size: 12px;
		  /*display: block;*/
		  /*margin-left: auto;*/
		  margin: 8px 0px;
		  margin-right: auto;
		  min-width:60px;
		}

		.playbutton:hover, .playbutton:active {
		  background-color: rgb(128, 176, 126);
		}
	</style>
</div>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:100%; min-width: 300px">
  <img src="/assets/scaling-down/constructing.png" id="demoDisplay" style="width:100%" />
  <button class="playbutton" id="demo_button" onclick="toggleDemo()">Play</button> 
</div>

<script language="javascript">
	function toggleDemo() {

		path = document.getElementById("demoDisplay").src
		var button = document.getElementById("demo_button");
	    if (path.split('/').pop() == "constructing.png") {
	        document.getElementById("demoDisplay").src = "/assets/scaling-down/constructing.gif";
	        button.textContent = "Pause";
	    } else {
	        document.getElementById("demoDisplay").src = "/assets/scaling-down/constructing.png";
	        button.textContent = "Play";
	    }
	}
</script>

<div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;">
	<a href="" id="linkbutton" target="_blank">Read the paper</a>
	<a href="https://bit.ly/3fghqVu" id="linkbutton" target="_blank">Run in browser</a>
	<a href="https://github.com/greydanus/mnist1d" id="linkbutton" target="_blank">Get the code</a>
</div>

<p>By any scientific standard, the Human Genome Project <a href="https://deepblue.lib.umich.edu/handle/2027.42/62798">was enormous</a>: it involved billions of dollars of funding, dozens of institutions, and over a decade of accelerated research. But that was only the tip of the iceberg. Long before the project began, scientists were hard at work assembling the intricate science of human genetics. And most of the time, they were not studying humans. The foundational discoveries in genetics centered on far simpler organisms such as peas, molds, fruit flies, and mice. To this day, biologists use these simpler organisms as genetic ‚Äúminimal working examples‚Äù in order to save time, energy, and money. A well-designed experiment with Drosophilia, such as <a href="https://pubmed.ncbi.nlm.nih.gov/10746727/">Feany and Bender (2000)</a>, can teach us an astonishing amount about humans.</p>

<p>The deep learning analogue of Drosophilia is the MNIST dataset. A large number of deep learning innovations including <a href="https://jmlr.org/papers/v15/srivastava14a.html">dropout</a>, <a href="https://arxiv.org/abs/1412.6980">Adam</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf">convolutional networks</a>, <a href="https://arxiv.org/abs/1406.2661">generative adversarial networks</a>, and <a href="https://arxiv.org/abs/1312.6114">variational autoencoders</a> began life as MNIST experiments. Once these innovations proved themselves on small-scale experiments, scientists found ways to scale them to larger and more impactful applications.</p>

<p>They key advantage of Drosophilia and MNIST is that they dramatically accelerate the iteration cycle of exploratory research. In the case of Drosophilia, the fly‚Äôs life cycle is just a few days long and its nutritional needs are negligible. This makes it much easier to work with than mammals, especially humans. In the case of MNIST, training a strong classifier takes a few dozen lines of code, less than a minute of walltime, and negligible amounts of electricity. This is a stark contrast to state-of-the-art vision, text, and game-playing models which can take months and hundreds of thousands of dollars of electricity to train.</p>

<p>Yet in spite of its historical significance, MNIST has three notable shortcomings. First, it does a poor job of differentiating between linear, nonlinear, and translation-invariant models. For example, logistic, MLP, and CNN benchmarks obtain 94, 99+, and 99+% accuracy on it. This makes it hard to measure the contribution of a CNN‚Äôs spatial priors or to judge the relative effectiveness of different regularization schemes. Second, it is somewhat large for a toy dataset. Each input example is a 784-dimensional vector and thus it takes a non-trivial amount of computation to perform hyperparameter searches or debug a metalearning loop. Third, MNIST is hard to hack. The ideal toy dataset should be procedurally generated so that researchers can smoothly vary parameters such as background noise, translation, and resolution.</p>

<div class="imgcap" style="display: block; margin-left: auto; margin-right: auto; width:99.9%">
  <div style="width:50%; min-width:250px; display: inline-block; vertical-align: top;">
    <img src="/assets/scaling-down/overview_a.png" style="width:100%" />
    <div style="text-align: left; padding:6px;padding-bottom: 20px;">Visualizing the construction of the MNIST-1D dataset. Like MNIST, the classifier's objective is to determine which digit is present in the input. Unlike MNIST, each example is a one-dimensional sequence of points. To generate an example, we begin with a digit template and then randomly pad, translate, and transform it.</div>
  </div>
  <div style="width:49.4%; min-width:250px; display: inline-block; vertical-align: top;">
    <img src="/assets/scaling-down/overview_b.png" style="width:100%" />
    <div style="text-align:left; padding:6px;">Visualizing the performance of common models on the MNIST-1D dataset. Whereas most ML models perform within a few percent accuracy on MNIST, this dataset separates them cleanly according to whether they use nonlinear features or not (logistic regression vs. MLP) or whether htey have spatial inductive biases (MLP vs. CNN).</div>
  </div>
</div>

<p>In order to address these shortcomings, we propose the MNIST-1D dataset. It is a minimalist, low-memory, and low-compute alternative to MNIST, designed for exploratory deep learning research where rapid iteration is a priority. Training examples are 20 times smaller but they are still better at measuring the difference between 1) linear and nonlinear classifiers and 2) models with and without spatial inductive biases (eg. translation invariance). The dataset is procedurally generated but still permits analogies to real-world digit classification.</p>

<h2 id="context">Context</h2>

<p>The machine learning community has grown rapidly in recent years. This growth has accelerated the rate of scientific innovation, but it has also produced multiple competing narratives about the field‚Äôs ultimate direction and objectives. In this section, we will explore three such narratives in order to place MNIST-1D in its proper context.</p>

<p>\textbf{Scaling trends.} One of the defining features of machine learning in the 2010‚Äôs was a <a href="https://openai.com/blog/ai-and-compute/">massive increase</a> in the scale of datasets, models, and compute infrastructure. This scaling pattern allowed neural networks to achieve breakthrough results on a wide range of benchmarks. Yet while this scaling effect has helped neural networks take on commercial and political relevance, opinions differ about how much more ‚Äúintelligence‚Äù it can generate. One one hand, many researchers and organizations argue that <a href="https://openai.com/blog/ai-and-compute/">scaling is a crucial path</a> to making neural networks behave more intelligently. On the other hand, there is a healthy but marginal population of researchers who are not primarily motivated by scale. They are united by a common desire to change research methodologies, advocating a shift away from human-engineered datasets and architectures \cite{clune2019ai}, an emphasis on human-like priors \cite{chollet2019measure}, and better integration with traditional symbolic AI approaches \cite{marcus2018deep}.</p>

<p>Once again, the genetics analogy is useful. In genetics, scale has been most effective when small-scale experiments have helped to guide the direction and vision of large-scale experiments. For example, the organizers of the Human Genome Project regularly used yeast and fly genomes to guide analysis of the human genome \cite{lander2001initial}. Thus one should be suspicious of research agendas that place disproportionate emphasis on large-scale experiments, since a healthy research ecosystem needs both. The fast, small scale projects permit creativity and deep understanding, whereas the large-scale projects expose fertile new research territory.</p>

<p>\textbf{Understanding vs. performance.} Researchers are also divided over the value of understanding versus performance. Some contend that a high-performing algorithm need not be interpretable so long as it saves lives or produces economic value. Others argue that hard-to-interpret deep learning models should not be deployed in sensitive real-world contexts. Both arguments have merit, but the best path forward seems to be to focus on understanding high-performing algorithms better so that this tradeoff becomes less severe. One way to do this is by identifying things we don‚Äôt understand about neural networks, reproducing these things on a toy problem like MNIST-1D, and then performing ablation studies to isolate the causal mechanisms.</p>

<p>\textbf{Ecological impacts.} A growing number of researchers and organizations claim that deep learning will have positive environmental applications \cite{loehle1987applying, rolnick2019tackling}. This may be true in the long run, but so far artificial intelligence has done little to solve environmental problems. In the meantime, deep learning models are consuming massive amounts of electricity to train and deploy \cite{strubell2019energy}. Our hope is that benchmarks like MNIST-1D will encourage researchers to spend more time iterating on small datasets and toy models before scaling, making more efficient use of electricity in the process.</p>

<h2 id="closing-thoughts">Closing thoughts</h2>

<p>There is a counterintuitive possibility that in order to explore the limits of how large we can scale neural networks, we may need to explore the limits of how small we can scale them first. Scaling models and datasets downward in a way that preserves the nuances of their behaviors at scale will allow researchers to iterate quickly on fundamental and creative ideas. This fast iteration cycle is the best way of obtaining insights about how to incorporate progressively more complex inductive biases into our models. We can then transfer these inductive biases across spatial scales in order to dramatically improve the sample efficiency and generalization properties of large-scale models. We see the humble MNIST-1D dataset as a tiny first step in that direction.</p>

<h2 id="footnotes">Footnotes</h2>
:ET